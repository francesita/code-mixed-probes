# code-mixed-probes
This reposotpry consists of the code and data used for the COLING-LREC 2024 paper 'Investigating Pre-Trained Model Generalisation with Code-Switched Text: Insights Using Code-Mixed Probes' by Frances Laureano De Leon, Harish Tayyar Madabushi, and Mark Lee. 

## PLMs used
1. bert-base-multilingual-uncased
2. bert-large-multilingual-uncased
3. xlm-roberta-base
4. xlm-roberta-large

## Data

The data folder contains the data used for each of the experiments presented in the paper. The data folder contains subfolders: 'semantics' , 'layer-wise', and 'syntactic'. The data in each subfolder corresponds to the title of each of the experiments mentioned in the paper. 



